{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubhamkumar.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367, 227, 227, 3)\n",
      "here2:  (2, 367)\n",
      "shape of input data:  (367, 227, 227, 3)\n",
      "shape of target variable:  (367, 2)\n",
      "(367, 227, 227, 3)\n",
      "here2:  (2, 367)\n",
      "shape of input data:  (367, 227, 227, 3)\n",
      "shape of target variable:  (367, 2)\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, _ = load_data('data')\n",
    "X_test, Y_test, _ = load_data('data')\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    X = tf.placeholder(tf.float32, [None, 227, 227, 3])\n",
    "    Y = tf.placeholder(tf.float32,[None, 2])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    tf.set_random_seed(1)\n",
    "    W1 = tf.get_variable(\"W1\", [11,11,3,96], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [5,5,96,256], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W3 = tf.get_variable(\"W3\", [3,3,256,384], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W4 = tf.get_variable(\"W4\", [3,3,384,384], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W5 = tf.get_variable(\"W5\", [3,3,384,256], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    \n",
    "    parameters = {\"W1\":W1,\n",
    "                 \"W2\":W2,\n",
    "                 \"W3\":W3,\n",
    "                 \"W4\":W4,\n",
    "                 \"W5\":W5,\n",
    "                 \"bc1\": tf.Variable(tf.constant(0.0, shape=[96]),        name=\"bc1\"),\n",
    "                 \"bc2\": tf.Variable(tf.constant(1.0, shape=[256]),       name=\"bc2\"),\n",
    "                 \"bc3\": tf.Variable(tf.constant(0.0, shape=[384]),       name=\"bc3\"),\n",
    "                 \"bc4\": tf.Variable(tf.constant(1.0, shape=[384]),       name=\"bc4\"),\n",
    "                 \"bc5\": tf.Variable(tf.constant(1.0, shape=[256]),       name=\"bc5\"),\n",
    "                 \"bf1\": tf.Variable(tf.constant(1.0, shape=[128]),       name=\"bf1\"),\n",
    "                 \"bf2\": tf.Variable(tf.constant(1.0, shape=[64]),        name=\"bf2\"),\n",
    "                 \"bf3\": tf.Variable(tf.constant(1.0, shape=[2]),         name=\"bf3\")}\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    W5 = parameters['W5']\n",
    "    bc1 = parameters['bc1']\n",
    "    bc2 = parameters['bc2']\n",
    "    bc3 = parameters['bc3']\n",
    "    bc4 = parameters['bc4']\n",
    "    bc5 = parameters['bc5']\n",
    "    bf1 = parameters['bf1']\n",
    "    bf2 = parameters['bf2']\n",
    "    bc3 = parameters['bc3']\n",
    "    \n",
    "    #layer1\n",
    "    Z1 = tf.nn.conv2d(X, W1, strides = [1,4,4,1], padding ='SAME')\n",
    "    Z1 = tf.nn.bias_add(Z1, bc1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    A1 = tf.nn.local_response_normalization(A1, depth_radius=5.0, bias=2.0, alpha=1e-4, beta=0.75)\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[1,3,3,1],strides=[1,2,2,1], padding='VALID')\n",
    "    #layer2\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides = [1,1,1,1], padding ='SAME')\n",
    "    Z2 = tf.nn.bias_add(Z2, bc2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    A2 = tf.nn.local_response_normalization(A2, depth_radius=5.0, bias=2.0, alpha=1e-4, beta=0.75)\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,3,3,1],strides=[1,2,2,1], padding='VALID')\n",
    "    #layer3\n",
    "    Z3 = tf.nn.conv2d(P2, W3, strides = [1,1,1,1], padding ='SAME')\n",
    "    Z3 = tf.nn.bias_add(Z3, bc3)\n",
    "    P3 = tf.nn.relu(Z3)\n",
    "    #A3 = tf.nn.local_response_normalization(A3, depth_radius=5.0, bias=2.0, alpha=1e-4, beta=0.75)\n",
    "    \n",
    "    #layer4\n",
    "    Z4 = tf.nn.conv2d(P3, W4, strides = [1,1,1,1], padding ='SAME')\n",
    "    Z4 = tf.nn.bias_add(Z4, bc4)\n",
    "    P4 = tf.nn.relu(Z4)\n",
    "\n",
    "    #layer5\n",
    "    Z5 = tf.nn.conv2d(P4, W5, strides = [1,1,1,1], padding ='SAME')\n",
    "    Z5 = tf.nn.bias_add(Z5, bc5)\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    P5 = tf.nn.max_pool(A5, ksize=[1,3,3,1],strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    P = tf.contrib.layers.flatten(P5)\n",
    "    #fully_connected_1\n",
    "    #Z3 = tf.contrib.layers.fully_connected(P,400,activation=tf.nn.relu) \n",
    "    fc1_w = tf.Variable(tf.truncated_normal(shape = (9216,128), mean = 0, stddev = 0.1))\n",
    "    fc1_b = tf.Variable(tf.zeros(128))\n",
    "    fc1 = tf.matmul(P,fc1_w) + fc1_b\n",
    "    Z6 = tf.nn.relu(fc1)\n",
    "    #fully_connected_2\n",
    "    fc2_w = tf.Variable(tf.truncated_normal(shape = (128,64), mean = 0, stddev = 0.1))\n",
    "    fc2_b = tf.Variable(tf.zeros(64))\n",
    "    fc2 = tf.matmul(Z6,fc2_w) + fc2_b\n",
    "    Z7 = tf.nn.relu(fc2)\n",
    "    #fully_connected_3\n",
    "    fc3_w = tf.Variable(tf.truncated_normal(shape = (64,2), mean = 0 , stddev = 0.1))\n",
    "    fc3_b = tf.Variable(tf.zeros(2))\n",
    "    Z8 = tf.matmul(Z7, fc3_w) + fc3_b\n",
    "    \n",
    "    return Z8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z8, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z8, labels=Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,X_test,Y_test,learning_rate=0.009,\n",
    "         num_epochs=16, minibatch_size=32,print_cost=True):\n",
    "    ops.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed=3\n",
    "    (m,n_H0,n_W0,n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    print(\"n_y: \", n_y)\n",
    "    costs=[]\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    Z8 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z8, Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            print(\"after epoch\")\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z8, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_y:  2\n",
      "WARNING:tensorflow:From <ipython-input-6-b6d242d76205>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
